
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital, %% This option enables the default options for the
           %% digital version of a document. Replace with `printed`
           %% to enable the default options for the printed version
           %% of a document.
  table,   %% Causes the coloring of tables. Replace with `notable`
           %% to restore plain tables.
  nolof,     %% Prints the List of Figures. Replace with lof/`nolof` to
           %% hide the List of Figures.
  nolot,     %% Prints the List of Tables. Replace with lot/`nolot` to
           %% hide the List of Tables.
  %% More options are listed in the user guide at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/guide/mu/fi.pdf>.
	draft, %% ked bude napisane, tak zmenit na final
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
\usepackage{paratype}
\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date          = \the\year/\the\month/\the\day,
    university    = mu,
    faculty       = fi,
    type          = mgr,
    author        = Adrian,
    gender        = m,
    advisor       = John Smith,
    title         = {Thesis title},
    TeXtitle      = {Thesis title},
    keywords      = {keyword1, keyword2, ...},
    TeXkeywords   = {keyword1, keyword2, \ldots},
    abstract      = {This is the abstract of my thesis, which can

                     span multiple paragraphs.},
    thanks        = {This is the acknowledgement for my thesis, which can

                     span multiple paragraphs.},
    bib           = bibliothesis.bib,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{listings} %% Source code highlighting
\lstset{
  basicstyle      = \ttfamily,%
  identifierstyle = \color{black},%
  keywordstyle    = \color{blue},%
  keywordstyle    = {[2]\color{cyan}},%
  keywordstyle    = {[3]\color{olive}},%
  stringstyle     = \color{teal},%
  commentstyle    = \itshape\color{magenta}}
%%begin, pridane
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{placeins}
\usepackage{pgfplots}
\usetikzlibrary{decorations.pathreplacing,angles,quotes}
\thesisload
%%end
\begin{document}
\chapter{Introduction}
Intro

\chapter{Chapter}
\section{Definitions}
\subsection{Univariate polynomial}
A univariate polynomial $f$ over a ring $R$ is a mathematical expression of the form
\begin{align}
       f = a_{n}x^{n}  +  a_{n-1}x^{n-1} +  \ldots  +  a_{1}x  &+  a_{0} \label{eq:polynom}
\end{align}
where the $a_{n}, a_{n-1}, \ldots, a_{1}, a_{0}$  are the coefficients of the polynomial and elements of $R$, and $x$ is called an indeterminate or a variable.  The highest $n \geq 0$ is called the degree of the polynomial (such $n$ exists, since the set $\{i \, | \, f_{i} \neq 0 \}$ is finite) and $a_{n} \neq 0$ is called the leading coefficient \parencite{rosicky07}. 

\subsection{Roots of a univariate polynomial}
Let R be a ring, $f = a_{n}x^{n}  +  a_{n-1}x^{n-1} +  \ldots  +  a_{1}x  +  a_{0}$ a polynomial of $R[x]$, $c \in R$. Then an element $a_{n}c^{n}  +  a_{n-1}c^{n-1} +  \ldots  +  a_{1}c  +  a_{0}$ is called a value of the polynomial and we denote it as $f(c)$.

Using this, we can create a polynomial function by mapping every element $x$ of $R$, to the result of a substitution $f(x) = a_{n}x^{n}  +  a_{n-1}x^{n-1} +  \ldots  +  a_{1}x  +  a_{0}$ \parencite{polynomialsChina}.

Let $f$ be a polynomial over $R$, $c \in R$. We say that $c$ is a root of the polynomial $f$ if $f(c) = 0$ \parencite{rosicky07}.

\section{Approximation of a root using iterative methods}
The iterative methods generally require knowledge of one or more initial guesses for the desired root(s) of the polynomial. This often poses a problem itself and there are techniques and methods for finding them. The simplest method for finding a guess is by looking at the plot of the polynomial, which is often not possible (e.g. when dealing with very complex and long polynomials). Some of these methods will be shown later and thus for this section, we will assume we already have a guess.

\subsection{Bisection method}
The simplest method for finding a better approximation to a root is \textbf{bisection method}. Assume that function $f(x)$ is continuous on interval $[a,b]$ and that $f(a)f(a) < 0$. Then according to the intermediate value theorem \parencite{interValue} there must be at least one root in $[a,b]$. The interval may be chosen large enough that there is more than one root, this is not a problem however, since the algorithm will always converge to some root $\alpha$ in $[a,b]$ and a smaller interval containing only one root. Since all polynomial functions are continuous \parencite{polyCont}, we can use this theorem to create an algorithm.

\newcommand*\Let[2]{\State #1 $\gets$ #2}
\algrenewcommand\algorithmicrequire{\textbf{Precondition:}}
\algrenewcommand\algorithmicensure{\textbf{Postcondition:}}
\begin{algorithm}
  \caption{Bisection algorithm
    \label{alg:bisect}}
  \begin{algorithmic}[1]
    \Require{$f$ polynomial function, $a$, $b$ interval bounds, $\epsilon$ precision error}
    \Statex
    \Function{Bisection}{$f, a, b, \epsilon$}
      \Let{$c$}{$\frac{a + b}{2}$}
      \If{$c - a \leq \epsilon$}
				\Return{$c$}
			\EndIf
      \If{$f(a) * f(c) < 0$}
				\State \Return{\Call{Bisection}{$f, a, c, \epsilon$}}
				\Else \State \Return{\Call{Bisection}{$f, c, b, \epsilon$}}
			\EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\begin{example}
Find a root $\rho$ of 
\begin{align}
      2x^{4} - 3x - 2
\end{align}
with the precision $\epsilon = 0.00005$.
\end{example}
It is fairly straightforward to show that there is a root located between $1 < \rho < 2$, so we will use this interval as our initial guess. The results are shown in the table below.
%vysvetlit preco je to straightforward
\begin{table}
  \begin{tabularx}{\textwidth}{lll}
    \toprule
    Iteration & $c_{n}$ & $f(c_{n})$\\
    \midrule
    1 & 1.50000 & 3.62500 \\
    2 & 1.25000 & -0.86719 \\
    3 & 1.37500 & 1.023931 \\
    4 & 1.31250 & -0.00241 \\
    5 & 1.34375 & 0.48960 \\
    6 & 1.32813 & 0.23842 \\
    7 & 1.32031 & 0.11674 \\
    8 & 1.31641 & 0.05685 \\
    9 & 1.31445 & 0.02715 \\
    10 & 1.31348 & 0.01235 \\
    11 & 1.31299 & 0.00497 \\
    12 & 1.31275 & 0.00129 \\
    13 & 1.31262 & -0.00055 \\
    14 & 1.31268 & 0.00037 \\
    15 & 1.31265 & -0.00009 \\
    \bottomrule
  \end{tabularx}
  \caption{Bisection algorithm on example todo:numbering of examples}
  \label{tab:bis}
\end{table}

\begin{figure}
\centering
\def\FunctionF(#1){2*(#1)^4-3*(#1)-2}%
\begin{tikzpicture}
\begin{axis}[
        axis y line=center,
        axis x line=middle, 
        axis on top=true,
        xmin=-0.75,
        xmax=2.25,
        ymin=-4.5,
        ymax=1.5,
        height=8.0cm,
        width=\linewidth,
        grid,
        xtick={-0.5,0,...,2},
        ytick={-4,...,1},
    ]
    \addplot [domain=-0.75:2, samples=50, mark=none, ultra thick, blue] {\FunctionF(x)};
    \node [left, blue] at (axis cs: 2.2,0.7) {$2x^4-3x-2$};
		\draw [decorate, thick, decoration={brace, mirror, amplitude=10pt, raise=0pt}](axis cs:1,-0.5) -- (axis cs:2,-0.5);
		\draw [decorate, thick, decoration={brace, mirror, amplitude=10pt, raise=12pt}](axis cs:1,-0.5) -- (axis cs:1.5,-0.5);
		\draw [decorate, thick, decoration={brace, mirror, amplitude=5pt, raise=24pt}](axis cs:1.25,-0.5) -- (axis cs:1.5,-0.5);
		\draw [decorate, thick, decoration={brace, mirror, amplitude=3pt, raise=32pt}](axis cs:1.25,-0.5) -- (axis cs:1.375,-0.5);
		\draw [decorate, thick, decoration={brace, mirror, amplitude=1pt, raise=37pt}](axis cs:1.25,-0.5) -- (axis cs:1.3125,-0.5);
\end{axis}
\end{tikzpicture}
\caption{First 5 iterations of bisection algorithm on the example} \label{fig:bisp}
\end{figure}
%\FloatBarrier

The true solution is
\begin{align}
      \rho \doteq 1.31265975467417
\end{align}
The true error is
\begin{align}
      |\rho - c_{15}| = 0.00000975467417
\end{align}
which is smaller than the required error bound ($0.00005$). Upon closer inspection, it can be noticed that the algorithm found a solution with enough precision in an earlier iteration already (for example $c_{13}$) and it may seem as the computation could have been stopped right then. However, this fact was not known beforehand, because there is no possibility to predict the accuracy in an earlier iteration during computation.

While every iteration gives a better approximation of the true solution, the algorithm is converging rather slowly. To examine the speed of convergence, we first need to characterize it.

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\begin{definition}
  A sequence of iterates $\{ x_{n} | n \geq 0\}$ is said to converge with \textit{order $p \geq 1$} to a point $\alpha$ if
	\begin{align}
      |\alpha - x_{n + 1}| \leq c |\alpha - x_{n}|^{p}  \tag*{$n \geq 0$} 
	\end{align}
	for some $c > 0$. If $p = 1$, the sequence is said to \textit{converge linearly} to $\alpha$. In that case, we require $c < 1$; the constant $c$ is called the \textit{rate of linear convergence} of $x_{n}$ to $\alpha$ \parencite[p.~56]{rootApproxMeth}.
	\label{def:converg}
\end{definition}
Let $c_{n}$ denote the $n$th value of $c$ in the algorithm. Then 
\begin{align}
      \rho = \lim_{n\to\infty} c_{n}
\end{align}
\begin{align}
      |\rho - c_{n}| \leq \left[\frac{1}{2}\right]^n (b - a)
\end{align}
where $b - a$ denotes the length of the original interval input into the algorithm. Using our definition, we can say that the bisection algorithm has linear convergence with a rate of $\frac{1}{2}$. That does not necessarily mean that in every iteration the actual error decreases by a factor of $\frac{1}{2}$, but that the \textit{average} rate of decrease is $\frac{1}{2}$. This means that it takes on average approximately $3.32$ iterations ($\log_2 10$) to compute a single digit.

The major drawback of this algorithm is its very slow rate of convergence, specifically compared to other methods described in the following sections.

On the other hand, the \textit{Bisection algorithm} has several advantages. The first of them being that it is guaranteed to converge if the prerequisites are met(i.e. the $f$ is continuous - which is true for all polynomial functions - and $f(a)f(a) < 0$). The second one being a reasonable error bound. This method also provides upper and lower bounds on the root $\rho$ in every iteration and such belongs in class of methods called \textit{enclosure methods} \parencite{rootApproxMeth}.


\subsection{Newton's method}
\textbf{Newton's method}(sometimes also called \textbf{Newton-Raphson method}), named after Isaac Newton and Joseph Raphson, is another method for finding better approximations to a root of a real polynomial function (or in general, any real-valued function). The basic idea of the method is based on the fact that given a starting point $x_{0}$, that is sufficiently close to the root, one can approximate the function by computing its tangent line at the point $(x_{0}, f(x_{0}))$. Then, one can use the $x$-intercept of the tangent line, which typically provides a better approximation, and repeat this process ad infinitum \parencite{rootApproxMeth} (or until sufficient precision is reached). 

Assume that real-valued function $f(x)$ is differentiable on interval $[a,b]$ and that we have an initial approximation $x_{0}$. Then, using calculus, we can derive the formula for a better approximation $x_{1}$.

%obrazok s grafom a dotycnicou

To compute the better approximation $x_{1}$ we need to use the formula that describes the slope $m$ of our tangent line
\begin{align}
      m = \frac{f(x_{1}) - f(x_{0})}{x_{1} - x_{0}}
\end{align}
Substituting $f(x_{1})$ by $0$ and manipulating the equation, we obtain
\begin{align}
      x_{1} = x_{0} - \frac{f(x_{0})}{m}
\end{align}
And since the slope $m$ of the tangent line is the derivative of the function $f$ at the point $x_{0}$, we obtain our desired formula 
\begin{align}
      x_{1} = x_{0} - \frac{f(x_{0})}{f'(x_{0})}
\end{align}
The general formula is obtained by iterating the process, by replacing $x_{0}$ with $x_{1}$, ad infinitum, which gives us 
\begin{align}
      x_{n+1} = x_{n} - \frac{f(x_{n})}{f'(x_{n})} \label{eq:newt}
\end{align}
%pridat zdroj/proof ze polynomy su d.
Since all polynomial are differentiable, we can apply this formula to our problem. Newton's method is exceptionally powerful and one of the most well known techniques for finding the roots, since its rather easy to implement and converges very quickly. 

\newtheorem{theorem}{Theorem}[section] %% The numbering of theorems will be reset after each section.
\begin{theorem}
  Assume that $f(x), f'(x)$, and $f''(x)$ are all continuous for every $x$ in some neighbourhood $I = [\alpha - \epsilon, \alpha + \epsilon]$ of $\alpha$, $\alpha$ is a root of $f$, and $f'(x) \neq 0, \forall x \in I$. Then if $x_{0}$ is chosen sufficiently close to $\alpha$, the iterates $x_{n}, n \geq 0$ of \eqref{eq:newt} will converge to $\alpha$ \parencite[p.~60]{rootApproxMeth}. 
\end{theorem}
Furthermore, Atkinson\cite{rootApproxMeth} shows that the convergence is quadratic.
\begin{proof}
First, since $f$ is twice continuously differentiable around the root $\alpha$, we can use a Taylor series expansion\parencite[p.~59]{rootApproxMeth} to second order about a point close to $\alpha$, $x_{n}$, to represent $f(\alpha)$. The expansion of $f(\alpha)$ about $x_{n}$ is 
\begin{align}
	f(\alpha) = f(x_{n}) + f'(x_{n})(\alpha - x_{n}) + \frac{f''(\xi_{n})}{2!}(\alpha - x_{n})^2
\end{align}
where $\xi_{n}$ is between $x$ and $\alpha$. Since $\alpha$ is root, we can replace $f(\alpha)$ with $0$ and by manipulating the equation we get
\begin{align}
  -\alpha + x_{n}	- \frac{f(x_{n})}{f'(x_{n})} = \frac{f''(\xi_{n})}{2f'(x_{n})}(\alpha - x_{n})^2
\end{align}
where we can use the definition of $x_{n+1}$ from \eqref{eq:newt} and multiply the equation by $-1$ to get
\begin{align}
  \alpha - x_{n+1} = \frac{-f''(\xi_{n})}{2f'(x_{n})}(\alpha - x_{n})^2 
\end{align}
Taking the absolute value of both sides of the equation we get
\begin{align}
  \left|\alpha - x_{n+1}\right| = \frac{\left|f''(\xi_{n})\right|}{\left|2f'(x_{n})\right|}\left|\alpha - x_{n}\right|^2 
	\label{eq:newtcp}
\end{align}
Now we pick the sufficiently small interval $I = [\alpha - \epsilon, \alpha + \epsilon]$ on which $f'(x) \neq 0$ (which exists since $f'(x)$ is continuous) and then let
\begin{align}
  M = sup_{x\in I}\frac{\left|f''(x)\right|}{\left|2f'(x)\right|}
\end{align}
Pick $x_{0}$, such that $\left|\alpha-x_{0}\right| \leq \epsilon$ and $M\left|\alpha-x_{0}\right| < 1$\parencite[p.~61]{rootApproxMeth}. Then $M\left|\alpha - x_{1}\right| < 1$ and $M\left|\alpha - x_{1}\right| \leq M\left|\alpha - x_{0}\right|$. Using induction we can apply this argument for every $n \geq 1$, showing that $\left|\alpha-x_{n}\right| \leq \epsilon$ and $M\left|\alpha-x_{n}\right| < 1$ for all $n \geq 1$.
This, in combination with \eqref{eq:newtcp}, gives us
\begin{align}
  \left|\alpha - x_{n+1}\right| &\leq M\left|\alpha - x_{n}\right|^2 \\
	M\left|\alpha - x_{n+1}\right| &\leq (M\left|\alpha - x_{n}\right|)^2 
\end{align}
and
\begin{align}
  M\left|\alpha - x_{n}\right| &\leq (M\left|\alpha - x_{0}\right|)^{2^n} \\
	M\left|\alpha - x_{n}\right| &\leq \frac{1}{M}(M\left|\alpha - x_{0}\right|)^{2^n}
\end{align}
Since $M\left|\alpha-x_{0}\right| < 1$, this shows that $x_{n}\rightarrow\alpha$ as $n\rightarrow\infty$.
The aforementioned point $\xi_{n}$ is between $x_{n}$ and $\alpha$, which implies that $\xi_{n}\rightarrow\alpha$ as $n\rightarrow\infty$. Thus \parencite[p.~61]{rootApproxMeth}
\begin{align}
 \lim_{n\to\infty} \frac{\alpha - x_{n+1}}{(\alpha - x_{n})^2} = -\lim_{n\to\infty} \frac{f''(\xi_{n})}{2f'(x_{n})} = \frac{-f''(\alpha)}{2f'(\alpha)}
\end{align}
\end{proof}
%reworknut proof
However, this method has several deficiencies.

Firstly, just by observing the equation, we can see that if the iteration point is stationary, then $x_{n+1}$ is undefined, since $f'(x_{n}) = 0$ (which means the tangent line is parallel to the $x$-axis). 
%obrazok, example

Secondly, for some polynomial functions, the method may enter an infinite cycle. This happens if, for example, $x_{1}$ produces $x_{2} = x_{0}$ as output, causing the method to alternate between these two results infinitely.
%obrazok, example, vyriesi sa pridanim max.pocet iteracii

Thirdly, if the initial guess is not close enough or the first derivative does not behave well in the neighbourhood of a specific root, the method may skip this root as the tangent line will intercept the $x$-axis close to another root. This may pose a problem, if someone is looking for a root located specifically in an interval $[a,b]$, but does not pose a problem if one is simply looking for any root of a polynomial function.
%obrazok, example, neskor sa vyriesi

Lastly, if the root of the polynomial function has a multiplicity greater than one(i.e. the first derivative of the root is also zero), then the convergence to this root is only linear.
\begin{proof}
Let $f(x)$ be a polynomial function with a root $\alpha$ of multiplicity $m$, $m > 1$, i.e. $f(x) = (x-\alpha)^{m}g(x)$. Assume that $x_{0}$ is sufficiently close to $\alpha$. Then
\begin{align}
x_{n+1} &= x_{n} - \frac{(x_{n} - \alpha)^{m}g(x_{n})}{m(x_{n}-\alpha)^{m-1}g(x_{n}) + (x_{n}-\alpha)^{m}g'(x_{n})} \\
&= x_{n} - \frac{(x_{n} - \alpha)g(x_{n})}{mg(x_{n}) + (x_{n}-\alpha)g'(x_{n})} \\
&= \frac{x_{n}(mg(x_{n}) + (x_{n}-\alpha)g'(x_{n})) - (x_{n} - \alpha)g(x_{n})}{mg(x_{n}) + (x_{n}-\alpha)g'(x_{n})} \\
&= \frac{x_{n}(m - 1)g(x_{n}) + x_{n}(x_{n}-\alpha)g'(x_{n}) + {\alpha}g(x_{n})}{mg(x_{n}) + (x_{n}-\alpha)g'(x_{n})}
\end{align}
which, as we get closer to the root, i.e. $x_{n} \doteq \alpha$, gives us
\begin{align}
x_{n+1} &\doteq x_{n}\frac{(m - 1)}{m}+\frac{\alpha}{m}
\end{align}
or put differently
\begin{align}
x_{n+1} &\doteq (x_{n}-\alpha)\frac{(m - 1)}{m}+\alpha
\end{align}
from which we can conclude
\begin{align}
\left|\alpha - x_{n+1}\right| \leq \frac{(m - 1)}{m}\left|\alpha-x_{n}\right|
\end{align}
which by the definition \ref{def:converg} proves the linear convergence.
\end{proof}

%pridat error bound, potom pseudokod, example, napisat ze je ovela rychlejsi nez bisect, napisat ze patri medzi householder.

%Using this an algorithm.

%\begin{algorithm}
%  \caption{Newton's algorithm
%    \label{alg:newton}}
%  \begin{algorithmic}[1]
%    \Require{$f$ polynomial function, $a$, $b$ interval %bounds, $\epsilon$ precision error}
%    \Statex
%    \Function{Newton}{$f, a, b, \epsilon$}
%      \Let{$c$}{$\frac{a + b}{2}$}
%      
%    \EndFunction
%  \end{algorithmic}
%\end{algorithm}

\printbibliography[heading=bibintoc]


  \makeatletter\thesis@blocks@clear\makeatother
  \phantomsection %% Print the index and insert it into the
  \addcontentsline{toc}{chapter}{\indexname} %% table of contents.
  \printindex

\appendix %% Start the appendices.
\chapter{An appendix}
Here you can insert the appendices of your thesis.

\end{document}
