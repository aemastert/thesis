
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital, %% This option enables the default options for the
           %% digital version of a document. Replace with `printed`
           %% to enable the default options for the printed version
           %% of a document.
  table,   %% Causes the coloring of tables. Replace with `notable`
           %% to restore plain tables.
  nolof,     %% Prints the List of Figures. Replace with lof/`nolof` to
           %% hide the List of Figures.
  nolot,     %% Prints the List of Tables. Replace with lot/`nolot` to
           %% hide the List of Tables.
  %% More options are listed in the user guide at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/guide/mu/fi.pdf>.
	draft, %% ked bude napisane, tak zmenit na final
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
   czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%

%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
\usepackage{paratype}
\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date          = \the\year/\the\month/\the\day,
    university    = mu,
    faculty       = fi,
    type          = mgr,
    author        = {Adri{\'a}n Elgy{\"u}tt},
    gender        = m,
    advisor       = {RNDr. Vojt{\v e}ch {\v R}eh{\'a}k, Ph.D},
    title         = {Root Isolation of High-Degree Polynomials},
    TeXtitle      = {Root Isolation of High-Degree Polynomials},
    keywords      = {keyword1, keyword2, ...},
    TeXkeywords   = {keyword1, keyword2, \ldots},
    abstract      = {This is the abstract of my thesis, which can

                     span multiple paragraphs.},
    thanks        = {This is the acknowledgement for my thesis, which can

                     span multiple paragraphs.},
    bib           = bibliothesis.bib,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{listings} %% Source code highlighting
\lstset{
  basicstyle      = \ttfamily,%
  identifierstyle = \color{black},%
  keywordstyle    = \color{blue},%
  keywordstyle    = {[2]\color{cyan}},%
  keywordstyle    = {[3]\color{olive}},%
  stringstyle     = \color{teal},%
  commentstyle    = \itshape\color{magenta}}
%%begin, pridane
\usepackage{placeins}
\usepackage{pgfplots}
\usetikzlibrary{decorations.pathreplacing,angles,quotes}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\hypersetup{final=true}
\thesisload
%%end
\begin{document}
\newtheorem{theorem}{Theorem}[section] %% The numbering of theorems will be reset after each section.
\newtheorem{corollary}[theorem]{Corollary}
\chapter{Introduction}
Intro

\chapter{Approximation of a single root}
\section{Definitions}
\subsection{Univariate polynomial}
A univariate polynomial $f$ is a mathematical expression of the form
\begin{align}
       f = a_{n}x^{n}  +  a_{n-1}x^{n-1} +  \ldots  +  a_{1}x  &+  a_{0} \label{eq:polynom}
\end{align}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
where the $a_{n}, a_{n-1}, \ldots, a_{1}, a_{0}$  are the coefficients of the polynomial, $n$ is any nonnegative integer and $x$ is called an indeterminate or a variable.  The highest $n \geq 0$ is called the degree of the polynomial (such $n$ exists, since the set $\{i \, | \, a_{i} \neq 0 \}$ is finite) and $a_{n} \neq 0$ is called the leading coefficient. If every $a_{k}$, $0\leq{k}\leq{n}$, is a real number, we say that $f$ is polynomial over $\R$ or simply real polynomial.

\subsection{Roots of a univariate polynomial}
Let $f = a_{n}x^{n}  +  a_{n-1}x^{n-1} +  \ldots  +  a_{1}x  +  a_{0}$ be a real polynomial and $c\in\R$. Then an element $a_{n}c^{n}  +  a_{n-1}c^{n-1} +  \ldots  +  a_{1}c  +  a_{0}$ is called a value of the polynomial and we denote it as $f(c)$.

Using this, we can create a polynomial function by mapping every element $x\in\R$, to the result of $f(x) = a_{n}x^{n}  +  a_{n-1}x^{n-1} +  \ldots  +  a_{1}x  +  a_{0}$ \parencite{polynomialsChina}.

Let $f$ be a polynomial over $R$, $c\in\R$. We say that $c$ is a root of the polynomial $f$ if $f(c) = 0$ \parencite{rosicky07}.

\section{Approximation of a root using iterative methods}
The iterative methods generally require knowledge of one or more initial guesses for the desired root(s) of the polynomial. This often poses a problem itself and there are techniques and methods for finding them. The simplest method for finding a guess is by looking at the plot of the polynomial, which is often not possible (e.g. when dealing with very complex and long polynomials). Some of these methods will be shown later and thus for this section, we will assume we already have a guess.

\subsection{Bisection method}
The simplest method for finding a better approximation to a root is \textbf{bisection method}. 
\begin{theorem}[Intermediate value theorem]
Let $f$ be a function on $\R$. Consider an interval $I=[a,b]$ such that $f$ is continuous on $I$ and $\lambda\in\R$ such that $\lambda$ lies between $f(a)$ and $f(b)$. Then there exists a $\gamma$, $\gamma\in[a,b]$, such that $f(\gamma)=\lambda$.
\end{theorem}
Now, assume a function $f(x)$ that is continuous on interval $[a,b]$ and that $f(a)\cdot f(b)<0$. Then according to the intermediate value theorem \parencite{interValue} there must be at least one root in $[a,b]$. The interval may be chosen large enough that there is more than one root, this is not a problem however, since the bisection algorithm will always converge to some root $\alpha$ in $[a,b]$ and a smaller interval containing only one root. Since all polynomial functions are continuous \parencite{polyCont}, we can use this theorem to create an algorithm.
\newcommand*\Let[2]{\State #1 $\gets$ #2}
\algrenewcommand\algorithmicrequire{\textbf{Precondition:}}
\algrenewcommand\algorithmicensure{\textbf{Postcondition:}}
\begin{algorithm}
  \caption{Bisection algorithm
    \label{alg:bisect}}
  \begin{algorithmic}[1]
    \Require{$f$ polynomial function, $a$, $b$ interval bounds, $\epsilon$ precision error}
    \Statex
    \Function{Bisection}{$f, a, b, \epsilon$}
      \Let{$x$}{$\frac{a + b}{2}$}
      \If{$x - a \leq \epsilon$}
				\State\Return{$c$}
			\EndIf
      \If{$f(a) \cdot f(x) < 0$}
				\State \Return{\Call{Bisection}{$f, a, x, \epsilon$}}
				\Else \State \Return{\Call{Bisection}{$f, x, b, \epsilon$}}
			\EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\begin{example}\label{exampleBis}
Find a root $\alpha$ of 
\begin{align}
      2x^{4} - 3x - 2
\end{align}
with the precision $\epsilon = 0.000001$.
\end{example}
It is fairly straightforward to show that there is a root located between $1 < \alpha < 2$, so we will use this interval as our initial guess. The iterations of the bisection algorithm are shown in the table \ref{tab:bis}.
%vysvetlit preco je to straightforward
\begin{table}
  \begin{tabularx}{\textwidth}{lll}
    \toprule
    Iteration & $x_{n}$ & $f(x_{n})$\\
    \midrule
		1 & 1.500000 & 3.625000 \\
		2 & 1.250000 & -0.867188 \\
		3 & 1.375000 & 1.023926 \\
		4 & 1.312500 & -0.002411 \\
		5 & 1.343750 & 0.489595 \\
		6 & 1.328125 & 0.238424 \\
		7 & 1.320313 & 0.116730 \\
		8 & 1.316406 & 0.056843 \\
		9 & 1.314453 & 0.027138 \\
		10 & 1.313477 & 0.012344 \\
		11 & 1.312988 & 0.004961 \\
		12 & 1.312744 & 0.001275 \\
		13 & 1.312622 & -0.000568 \\
		14 & 1.312683 & 0.000354 \\
		15 & 1.312653 & -0.000106 \\
		16 & 1.312668 & 0.000124 \\
		17 & 1.312660 & 0.000010 \\
		18 & 1.312657 & -0.000048 \\
		19 & 1.312659 & -0.000019 \\
		20 & 1.312660 & -0.000004 \\
    \bottomrule
  \end{tabularx}
  \caption{Bisection algorithm on Example \ref{exampleBis}}
  \label{tab:bis}
\end{table}

\begin{figure}
\centering
\def\FunctionF(#1){2*(#1)^4-3*(#1)-2}%
\begin{tikzpicture}
\begin{axis}[
        axis y line=center,
        axis x line=middle, 
        axis on top=true,
        xmin=-0.75,
        xmax=2.25,
        ymin=-4.5,
        ymax=1.5,
        height=8.0cm,
        width=\linewidth,
        grid,
        xtick={-0.5,0,...,2},
        ytick={-4,...,1},
    ]
    \addplot [domain=-0.75:2, samples=50, mark=none, ultra thick, blue] {\FunctionF(x)};
    \node [left, blue] at (axis cs: 2.2,0.7) {$2x^4-3x-2$};
		\draw [decorate, thick, decoration={brace, mirror, amplitude=10pt, raise=0pt}](axis cs:1,-0.5) -- (axis cs:2,-0.5);
		\draw [decorate, thick, decoration={brace, mirror, amplitude=10pt, raise=12pt}](axis cs:1,-0.5) -- (axis cs:1.5,-0.5);
		\draw [decorate, thick, decoration={brace, mirror, amplitude=5pt, raise=24pt}](axis cs:1.25,-0.5) -- (axis cs:1.5,-0.5);
		\draw [decorate, thick, decoration={brace, mirror, amplitude=3pt, raise=32pt}](axis cs:1.25,-0.5) -- (axis cs:1.375,-0.5);
		\draw [decorate, thick, decoration={brace, mirror, amplitude=1pt, raise=37pt}](axis cs:1.25,-0.5) -- (axis cs:1.3125,-0.5);
\end{axis}
\end{tikzpicture}
\caption{First 5 iterations of bisection algorithm on the example} \label{fig:bisp}
\end{figure}
%\FloatBarrier

The correct approximation is
\begin{align}
      \alpha \doteq 1.31265975467417
\end{align}
The error of the final iteration is
\begin{align}
      |\alpha - x_{20}| \doteq 0.00000024532583
\end{align}
which is smaller than the required error bound ($0.000001$). Upon closer inspection, it can be noticed that the algorithm already found a solution with enough precision in an earlier iteration (for example $x_{19}$) and it may seem as the computation could have been stopped right then. However, this fact was not known beforehand, because there is no possibility to predict the accuracy in an earlier iteration during computation.

\paragraph{Speed of convergence}
While every iteration gives a better approximation of the true solution, the algorithm is converging rather slowly. To examine the speed of convergence, we first need to characterize it.

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\begin{definition}
  Suppose we have a sequence of real numbers $x_{0}, x_{1}\ldots$ and a real number $\alpha$ such that for every $\epsilon\in\R$, $\epsilon>0$ there exists $k\in\N$ such that $\left|x_{k}-\alpha\right|<\epsilon$. Then the sequence is said to converge to a point $\alpha$, written as $\lim_{k\to\infty}x_{k}=\alpha$. The sequence is said to \textit{converge linearly} if
	\begin{align}
      \lim_{n\to\infty}\frac{|\alpha - x_{n + 1}|}{|\alpha - x_{n}|} = c 
	\end{align}
	for some $0 < c < 1$. The constant $c$ is called the \textit{rate of linear convergence} of $x_{n}$ to $\alpha$.
	The sequence is said to converge with \textit{order p > 1} if \begin{align}
      \lim_{n\to\infty}\frac{|\alpha - x_{n + 1}|}{|\alpha - x_{n}|^{p}} > 0.
	\end{align}
	\label{def:converg}
\end{definition}
Let $x_{n}$ denote the $n$th value of $x$ in the Algorithm \ref{alg:bisect}. Then 
\begin{align}
	\alpha &= \lim_{n\to\infty} x_{n} \\
	|\alpha - x_{n}| &\leq \left[\frac{1}{2}\right]^n \left|b - a\right| \\
			\lim_{n\to\infty}\frac{\left|\alpha - x_{n+1}\right|}{\left|\alpha - x_{n}\right|} &= \lim_{n\to\infty}\frac{2^{-\left(n+1\right)}\left|b - a\right|}{2^{-n}\left|b - a\right|} = \frac{1}{2}
\end{align}
where $\left|b - a\right|$ denotes the length of the original interval input into the algorithm. Using Definition \ref{def:converg}, we can say that the bisection algorithm has linear convergence with a rate of $\frac{1}{2}$. That does not necessarily mean that in every iteration the actual error decreases by a factor of $\frac{1}{2}$, but that the \textit{average} rate of decrease is $\frac{1}{2}$. This means that it takes on average approximately $3.32$ iterations ($\log_2 10$) to compute a single digit.

The major drawback of this algorithm is its very slow rate of convergence, specifically compared to other methods described in the following sections.

On the other hand, the \textit{Bisection algorithm} has several advantages. The first of them being that it is guaranteed to converge if the prerequisites are met(i.e. the $f$ is continuous - which is true for all polynomial functions - and $f(a)\cdot f(b) < 0$). The second one is the existence of a reasonable error bound. This method also provides upper and lower bounds on the root $\alpha$ in every iteration and such belongs in class of methods called \textit{enclosure methods} \parencite{rootApproxMeth}.


\subsection{Newton's method}
\textbf{Newton's method} (sometimes also called \textbf{Newton-Raphson method}), named after Isaac Newton and Joseph Raphson, is another method for finding better approximations to a root of a real polynomial function (or in general, any real-valued function). The basic idea of the method is based on the fact that given a starting point $x_{0}$, that is sufficiently close to the root, one can approximate the function by computing its tangent line at the point $(x_{0}, f(x_{0}))$. Then, one can use the $x$-intercept of the tangent line, which typically provides a better approximation, and repeat this process ad infinitum \parencite{rootApproxMeth} (or until sufficient precision is reached). 

Assume that real-valued function $f(x)$ is differentiable on interval $[a,b]$ and that we have an initial approximation $x_{0}$. Then, using calculus, we can derive the formula for a better approximation $x_{1}$ as follows.

To compute the better approximation $x_{1}$, we need to use the formula that describes the slope $m$ of our tangent line
\begin{align}
      m = \frac{f(x_{1}) - f(x_{0})}{x_{1} - x_{0}}.
\end{align}

Since we are interested in finding where the tangent line intersects the x-axis, we substitute $f(x_{1})$ by $0$ and manipulating the equation, we obtain
\begin{align}
      x_{1} = x_{0} - \frac{f(x_{0})}{m}.
\end{align}
And since the slope $m$ of the tangent line is the derivative of the function $f$ at the point $x_{0}$, we obtain our desired formula 
\begin{align}
      x_{1} = x_{0} - \frac{f(x_{0})}{f'(x_{0})}.
\end{align}
The general formula is obtained by iterating the process, by replacing $x_{0}$ with $x_{1}$, ad infinitum, which gives us 
\begin{align}
      x_{n+1} = x_{n} - \frac{f(x_{n})}{f'(x_{n})}. \label{eq:newt}
\end{align}
%pridat zdroj/proof ze polynomy su d.
Since all polynomial functions are differentiable, we can apply this formula to our problem. Newton's method is exceptionally powerful and one of the most well known techniques for finding the roots, since its rather easy to implement and converges very quickly. 
\paragraph{Speed of convergence}
\begin{theorem}[{\parencite[p.~60]{rootApproxMeth}}]
  Assume that $f(x), f'(x)$, and $f''(x)$ are all continuous for every $x$ in some neighbourhood $I = [\alpha - \epsilon, \alpha + \epsilon]$ of $\alpha$, $\alpha$ is a root of $f$, and $f'(x) \neq 0, \forall x \in I$. Then if $x_{0}$ is chosen sufficiently close to $\alpha$, the iterates $x_{n}, n \geq 0$ of \eqref{eq:newt} will converge to $\alpha$. 
Furthermore,
\begin{align} 
 \lim_{n\to\infty} \frac{\alpha - x_{n+1}}{(\alpha - x_{n})^2} = \frac{-f''(\alpha)}{2f'(\alpha)} > 0
\end{align}
proving that the convergence is quadratic.
\end{theorem}
\begin{proof}
\textit{(Sketch)} First, since $f$ is twice continuously differentiable around the root $\alpha$, we can use a Taylor series expansion \parencite[p.~59]{rootApproxMeth} of second order about a point close to $\alpha$, $x_{n}$, to represent $f(\alpha)$. The expansion of $f(\alpha)$ about $x_{n}$ is 
\begin{align}
	f(\alpha) = f(x_{n}) + f'(x_{n})(\alpha - x_{n}) + \frac{f''(\xi_{n})}{2!}(\alpha - x_{n})^2 \label{eq:taylor1}
\end{align}
where $\xi_{n}$ is between $x$ and $\alpha$. Since $\alpha$ is root, we can replace $f(\alpha)$ with $0$ and by rewriting the equation we get
\begin{align}
  -\alpha + x_{n}	- \frac{f(x_{n})}{f'(x_{n})} = \frac{f''(\xi_{n})}{2f'(x_{n})}(\alpha - x_{n})^2
\end{align}
where we can use the definition of $x_{n+1}$ from \eqref{eq:newt} and multiply the equation by $-1$ to get
\begin{align}
  \alpha - x_{n+1} = \frac{-f''(\xi_{n})}{2f'(x_{n})}(\alpha - x_{n})^2 
\end{align}
Taking the absolute value of both sides of the equation we get
\begin{align}
  \left|\alpha - x_{n+1}\right| = \frac{\left|f''(\xi_{n})\right|}{\left|2f'(x_{n})\right|}\left(\alpha - x_{n}\right)^2 
	\label{eq:newtcp}
\end{align}
Now we pick the sufficiently small interval $I = [\alpha - \epsilon, \alpha + \epsilon]$ on which $f'(x) \neq 0$ (which exists since $f'(x)$ is continuous) and then let
\begin{align}
  M = sup_{x\in I}\frac{\left|f''(x)\right|}{\left|2f'(x)\right|}
\end{align}
Pick $x_{0}$, such that $\left|\alpha-x_{0}\right| \leq \epsilon$ and $M\left|\alpha-x_{0}\right| < 1$\parencite[p.~61]{rootApproxMeth}. Then $M\left|\alpha - x_{1}\right| < 1$ and $M\left|\alpha - x_{1}\right| \leq M\left|\alpha - x_{0}\right|$. Using induction we can apply this argument for every $n \geq 1$, showing that $\left|\alpha-x_{n}\right| \leq \epsilon$ and $M\left|\alpha-x_{n}\right| < 1$ for all $n \geq 1$.
This, in combination with \eqref{eq:newtcp}, gives us
\begin{align}
  \left|\alpha - x_{n+1}\right| &\leq M\left|\alpha - x_{n}\right|^2 \\
	M\left|\alpha - x_{n+1}\right| &\leq (M\left|\alpha - x_{n}\right|)^2 
\end{align}
and
\begin{align}
  M\left|\alpha - x_{n}\right| &\leq (M\left|\alpha - x_{0}\right|)^{2^n} \\
	\left|\alpha - x_{n}\right| &\leq \frac{1}{M}(M\left|\alpha - x_{0}\right|)^{2^n}
\end{align}
Since $M\left|\alpha-x_{0}\right| < 1$, this shows that $x_{n}\rightarrow\alpha$ as $n\rightarrow\infty$.
The aforementioned point $\xi_{n}$ is between $x_{n}$ and $\alpha$, which implies that $\xi_{n}\rightarrow\alpha$ as $n\rightarrow\infty$. Thus \parencite[p.~61]{rootApproxMeth}
\begin{align}
 \lim_{n\to\infty} \frac{\alpha - x_{n+1}}{(\alpha - x_{n})^2} = -\lim_{n\to\infty} \frac{f''(\xi_{n})}{2f'(x_{n})} = \frac{-f''(\alpha)}{2f'(\alpha)}
\end{align}
\end{proof}
%reworknut proof
However, this method has several deficiencies.

Firstly, just by observing the equation, we can see that if the iteration point is stationary, then $x_{n+1}$ is undefined, since $f'(x_{n}) = 0$ (which means the tangent line is parallel to the $x$-axis). 
%obrazok, example

Secondly, for some polynomial functions, the method may enter an infinite cycle. This happens if, for example, $x_{1}$ produces $x_{1} = x_{0}$ as output, causing the method to alternate between these two results infinitely.
%obrazok, example, vyriesi sa pridanim max.pocet iteracii

Thirdly, if the initial guess is not close enough or the first derivative does not behave well in the neighbourhood of a specific root, the method may skip this root as the tangent line will intercept the $x$-axis close to another root. This may pose a problem, if someone is looking for a root located specifically in an interval $[a,b]$, but does not pose a problem if one is simply looking for any root of a polynomial function.
%obrazok, example, neskor sa vyriesi

Lastly, if the root (of the polynomial function) has a multiplicity greater than one(i.e. the first derivative of the root is also zero), then the convergence to this root is only linear.
\begin{proof}
Let $f(x)$ be a polynomial function with a root $\alpha$ of multiplicity $m$, $m > 1$, i.e. $f(x) = (x-\alpha)^{m}g(x)$. Assume that $x_{0}$ is sufficiently close to $\alpha$. Then
\begin{align}
x_{n+1} &= x_{n} - \frac{(x_{n} - \alpha)^{m}g(x_{n})}{m(x_{n}-\alpha)^{m-1}g(x_{n}) + (x_{n}-\alpha)^{m}g'(x_{n})} \\
&= x_{n} - \frac{(x_{n} - \alpha)g(x_{n})}{mg(x_{n}) + (x_{n}-\alpha)g'(x_{n})} \\
&= \frac{x_{n}(mg(x_{n}) + (x_{n}-\alpha)g'(x_{n})) - (x_{n} - \alpha)g(x_{n})}{mg(x_{n}) + (x_{n}-\alpha)g'(x_{n})} \\
&= \frac{x_{n}(m - 1)g(x_{n}) + x_{n}(x_{n}-\alpha)g'(x_{n}) + {\alpha}g(x_{n})}{mg(x_{n}) + (x_{n}-\alpha)g'(x_{n})}
\end{align}
which, as we get closer to the root, i.e. $x_{n} \doteq \alpha$, gives us
\begin{align}
x_{n+1} &\doteq x_{n}\frac{(m - 1)}{m}+\frac{\alpha}{m}
\end{align}
or put differently
\begin{align}
x_{n+1} &\doteq (x_{n}-\alpha)\frac{(m - 1)}{m}+\alpha
\end{align}
from which we can conclude
\begin{align}
\lim_{n\to\infty}\frac{\left|\alpha - x_{n+1}\right|}{\left|\alpha-x_{n}\right|} = \frac{(m - 1)}{m}
\end{align}
which by the definition \ref{def:converg} proves the linear convergence.
\end{proof}
\paragraph{Error bounds}
To estimate the error and provide error bounds of our computation, we will need to use the variation of the \textbf{mean value theorem}.
\subparagraph{Mean value theorem}
\begin{theorem}
Let $f$ be a real-valued polynomial function and $a, b$ real numbers, such that $a < b$. Then there exists some $c\in(a, b)$ such that
\begin{align}
f'(c) = \frac{f(b) - f(a)}{b - a}.
\end{align}
\end{theorem}%pridat odkaz na zdroj na proof
Roughly speaking, it states that given a curve, which starts at point $a$ and ends in point $b$, there is at least one point at which the tangent to the curve is parallel to the secant connecting the two points.
%vlozit obrazok
Using this theorem on a polynomial function $f$ with a root $\alpha$ and an approximation $x_{n}$, $\alpha < x_{n}$ we get 
\begin{align}
f'(\xi_n) = \frac{f(x_{n}-f(\alpha))}{x_{n}-\alpha}
\end{align}
where $\xi_{n}$ being between $\alpha$ and $x_{n}$. Since $\alpha$ is the root of $f$, then we can remove $f(\alpha)$ and simplifying we get
\begin{align}
\alpha - x_{n} = \frac{-f(x_{n})}{f'(\xi_n)}.
\end{align}
Similarly, if $x_{n} < \alpha$, we arrive to the same formula. If $f'(x)$ is not changing rapidly between $x_{n}$ and $\alpha$, then $f'(\xi_{n}) \doteq f'(x_{n})$. Combining this with the definition of Newton's method we get
\begin{align}
\alpha - x_{n} \doteq \frac{-f(x_{n})}{f'(x_{n})} = x_{n+1} - x_{n}.
\end{align}
This gives us the absolute error estimate
\begin{align}
\alpha - x_{n} \doteq x_{n+1} - x_{n} \label{eq:newtabser}
\end{align}
and relative error estimate
\begin{align}
\frac{\alpha - x_{n}}{\alpha} \doteq \frac{x_{n+1} - x_{n}}{x_{n+1}}.\label{eq:newtreler}
\end{align}
\paragraph{The Newton algorithm}
Using the Newton formula \eqref{eq:newt} and the error estimates \eqref{eq:newtabser}, \eqref{eq:newtreler} provided above, we can create the following algorithm.

\begin{algorithm}
  \caption{Newton's algorithm
    \label{alg:newton}}
  \begin{algorithmic}[1]
    \Require{$f$ polynomial function, $f'$ derivative of $f$, $a$, $b$ interval bounds, $\epsilon$ precision error, $max$ number of maximum iterations, $err$ error flag}
    \Statex
    \Function{Newton}{$f, f', a, b, \epsilon, max$}
			\Let{$err$}{\texttt{1}}
			\Let{$x_{0}$}{$\frac{a+b}{2}$}
			\For{$i\gets 1, max$}
				\Let{$denom$}{$f'(x_{0})$}
				\If{$denom=0$}
					\Let{$err$}{2}
					\State\Return{$err, x_{1}$}
				\EndIf
				\Let{$x_{1}$}{$x_{0}-\frac{f(x_{0})}{denom}$}
				\If{$\left(\left|\frac{x_{1}-x_{0}}{x_{1}}\right|\leq\epsilon\right)$ \textbf{and} $\left(\left|x_{1}-x_{0}\right|\leq\epsilon\right)$}
					\Let{$err$}{\texttt{0}}
					\State\Return{$success, x_{1}$}
				\EndIf
				\If{$(x_{1}<a)$ \textbf{or} $(x_{1}>b)$}
					\Let{$err$}{\texttt{3}}
					\State\Return{$err, x_{1}$}
				\EndIf
				\Let{$x_{0}$}{$x_{1}$}
			\EndFor
			\State\Return{$err, x_{1}$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}
The $max$ variable rules out the possibility of getting stuck in an endless loop, when the method oscillates between two points and does not converge (meaning there is no root located in the interval $(a,b)$). However, in the case that the function iterates through the maximum amount of iterations $max$ and returns no root (i.e. $i = max$ and $err$ is 1), but one knows the root of the $f$ is located in $(a,b)$, one may try to increase the $max$ or narrow the interval $(a,b)$ (e.g. using bisection) and try running the algorithm again.
 
The lines 12 to 13 serve as a prevention against the case of overshooting the root. These lines may be omitted if one does not have particular interest in finding the root from the specific interval (in this case, input interval $a,b$ may be switched for a single starting point $x_{0}$). As in previous case, if the algorithm ends prematurely because of the latest iteration being outside of the interval, but one is certain that the root is located inside the interval, one may try narrowing the interval (thus giving a better starting point).

The termination condition is a combination of the error bounds, both absolute and relative. The reason for this choice is that while relative error works well in small magnitude and worse when the root is large in magnitude, the opposite is true for absolute error. That is because for error $\epsilon = 10^{-e}$ the relative error of \eqref{eq:newtreler} gives at least $e-1$ correct digits. This means that for an approximation in the floating point representation at least $e-1$ digits of the significand are correct. Whereas the absolute error of \eqref{eq:newtabser} provides at least $e-1$ correct digits after the decimal point, when the number is not in the floating point representation. 

E.g. Let $f(x)=x^3-100x^2+x-100$, which has a root $\alpha=100$ and set $\epsilon=0.1$. Then, using only the relative error bound and setting $a=0, b=2000$, the algorithm returns as a result $x=101.5$ after 7 iterations, ending too early. Using the absolute error bound, the algorithm ends after 9 iterations with the desired result $x=100.0$. 
Now let $f(x)=x^3-0.0001x^2+x-0.0001$ which has a root $\alpha=0.0001$ and set $\epsilon=0.00001$. Then, using only the absolute error bound and setting $a=0, b=100$, the algorithm returns as a result $=x0.000104$ after 13 iterations. Then, using only the relative error bound and setting $a=0, b=100$, the algorithm returns as a result $x=0.0001000$ after 14 iterations. While both results are within the precision error, the relative error gives a more precise result. Thus the best results are achieved combining both bounds.
\begin{example}
\label{exampleNewt}
Find a root $\alpha$ of 
\begin{align}
      2x^{4} - 3x - 2
\end{align}
with the precision $\epsilon = 0.000001$.
\end{example}
The initial interval is the same as in bisection algorithm to make results comparable.
\FloatBarrier
\begin{table}[H]
  \begin{tabularx}{\textwidth}{lll}
    \toprule
    Iteration & $c_{n}$ & $f(c_{n})$\\
    \midrule
			1 & 1.500000 & 3.625000 \\
			2 & 1.348958 & 0.575658 \\
			3 & 1.314358 & 0.025697 \\
			4 & 1.312664 & 0.000060 \\
			5 & 1.312660 & 0.000001 \\
    \bottomrule
  \end{tabularx}
  \caption{Newton's algorithm on Example \ref{exampleNewt}}
  \label{tab:newt}
\end{table}

With correct approximation being $\alpha \doteq 1.31265975467417$, the error of the final iteration is $|\alpha - x_{5}|\doteq0.00000024532583$ which is within the allowed precision error.

While the algorithm solves some of the deficiencies of the method mentioned before, some remain. The method might sometimes fail (when the first derivative of $x_{n}$ is zero) and the convergence for the roots of higher multiplicity is only linear.

On the other hand, as we can see from the table, the algorithm converges much faster than the previous method, finding the sufficient approximation in only 5 iterations. This makes Newton's method superior to the bisection method in most of the cases.

\subsection{Halley's method}
\textbf{Halley's method}, named after its creator Edmond Halley, is a method that in addition to first derivative uses a second derivative of the function as well. While Newton's method could be geometrically expressed as a series of tangent lines which roots (x-intercepts) converge to the root of function, there is no such obvious interpretation for Halley's method. However, as Newton's method can be derived via a first order Taylor polynomial, Halley's method can be derived via a second order Taylor polynomial \parencite{halley},
\begin{align}
y(x)=f(x_{n})+f'(x_{n})(x-x_{n})+\frac{f''(x_{n})}{2!}(x-x_{n})^{2},
\end{align}
where $x_{n}$ is an approximation of $x$, such that $f(x) = 0$. As the objective is to calculate a point $x_{n+1}$ where the $y$ function intersects x-axis, we set $y(x)=0$ and the objective is then solving the equation
\begin{align}
0=f(x_{n})+f'(x_{n})(x_{n+1}-x_{n})+\frac{f''(x_{n})}{2}(x_{n+1}-x_{n})^{2}
\end{align}
for $x_{n+1}$. Following \cite{10.2307/2975033} and simplifying the equation to express $x_{n+1}$ on the left side we obtain
\begin{align}
x_{n+1}=x_{n}-\frac{f(x_{n})}{f'(x_{n})+\frac{f''(x_{n})}{2}(x_{n+1}-x_{n})}.
\end{align}
Substituting the difference $x_{n+1}-x_{n}$ with $-\frac{f(x_{n})}{f'(x_{n})}$ defined in \eqref{eq:newt}, we obtain
\begin{align}
x_{n+1}=x_{n}-\frac{2f(x_{n})f'(x_{n})}{2f'(x_{n})^{2}-f(x_{n})f''(x_{n})},
\end{align}
known as Halley's method.
\paragraph{Speed of convergence}
The Halley's method converges to the root cubically \parencite{halleyCnvg}, \parencite{10.2307/2975033} compared to Newton's quadratic convergence. The proof of cubic convergence follows similar fashion as Newton's, using Taylor series expansion. Despite the fact that the method converges faster than Newton's method, this is offset by the fact that the computation needed in every step is more complex than in Newton's method. If the computation of the derivatives is complicated (e.g. the polynomial has very high degree) or evaluation of $f(x), f'(x)$ and $f''(x)$ takes significant amount of time, the total amount of time may be similar or even higher to Newton's method.
In addition, if the second derivative is close to or exactly $0$ then the convergence speed is very similar to Newton's method.
\paragraph{Halley's algorithm}
The Halley's algorithm has similar structure to Newton's algorithm \ref{alg:newton}, only replacing lines 5 and 9 with the new equation.
\begin{example}
Find a root $\alpha$ of 
\begin{align}
      2x^{4} - 3x - 2
\end{align}
with the precision $\epsilon = 0.000001$.
\end{example}
The initial interval remains the same as in the previous algorithms to make the results comparable.
\FloatBarrier
\begin{table}[H]
  \begin{tabularx}{\textwidth}{lll}
    \toprule
    Iteration & $c_{n}$ & $f(c_{n})$\\
    \midrule
				1 & 1.318039 & 0.081800 \\
				2 & 1.312660 & 0.000001 \\
    \bottomrule
  \end{tabularx}
  \caption{Halley's algorithm on example}
  \label{tab:hall}
\end{table}
The algorithm finishes with the same result as the result provided by the bisection algorithm \ref{tab:bis} and Newton's algorithm \ref{tab:newt} while ending in only 2 iterations, as opposed to 21 in \ref{tab:bis} and 5 in \ref{tab:newt}.
The Halley's algorithm has the same disadvantages as Newton's algorithm, while providing a higher speed of convergence, at the cost of higher computation time in every iteration. Therefore it is preferable when the degree of the polynomial is rather low, but at higher degrees or when second derivative is very close to 0 (with respect to error tolerance), it doesn't provide significant advantage over Newton's method (and may even result in higher total time).

\chapter{Solving polynomials}
While the previous chapter focused on getting better approximation of a single root, this chapter focuses on solving polynomial functions and isolating the roots.
\section{Ruffini-Horner's method}
The \textbf{Ruffini-Horner's} method relies on what is known as \textit{fundamental theorem of algebra}.
\begin{theorem}[Fundamental theorem of algebra]
Given any positive integer $n \geq 1$ and any univariate polynomial $f$ with complex coefficients such that the degree of $f$ is $n$, the polynomial $f$ has at least 1 complex root.
\end{theorem}
This theorem has few important corollaries.
\begin{corollary}
Every univariate polynomial $f$ of degree $n \geq 1$ has exactly $n$ complex roots.
\end{corollary}
\begin{corollary}
Any univariate polynomial $f$ of degree $n \geq 1$ can be factored as $f=(x-z_{1})(x-z_{2})\ldots(x-z_{n})$, where $z_{1},\ldots,z_{n}$ are the complex roots.
\end{corollary}
\begin{corollary}
Any real univariate polynomial $f$ of degree $n \geq 1$ can be factored as $f=(x-z_{1})(x-z_{2})\ldots(x-z_{m})g(x)$, where $z_{1},\ldots,z_{m}$ are the real roots of $f$, $m\leq n$ and $g(x)$ is irreducible in $\R$.
\end{corollary}
The implication of this corollary is that we can start by finding any root $\alpha$ of $f(x)$, then divide $f(x)$ by factor $(x-\alpha)$ to obtain $f_{1}(x)$ such that $f(x)=(x-\alpha)f_{1}(x)$ and repeat the process for $f_{1}(x)$ until we reach $f_{m}(x)$ such that it is irreducible in $\R$ (i.e. has no real roots).

Because in each step we are dividing $f_{i}(x)$, $0 \leq i < m$ by a linear monic polynomial, we can use the efficient technique for the division known as \textit{Ruffini's rule} or \textit{Horner's method}, described in \parencite{horner}. This gives us a simple algorithm.

\begin{algorithm}
  \caption{Ruffini-Horner's algorithm
    \label{alg:rhn}}
  \begin{algorithmic}[1]
    \Require{$f$ polynomial function, $x_0$ initial guess, $\epsilon$ precision error, $max$ max amount of iter. for Newton}
    \Statex
    \Function{RuffiniHorner}{$f, x_0, \epsilon, max$}
			\Let{$roots$}{$\emptyset$}
			\Let{$f_{0}$}{$f$}
			\Let{$remainder$}{0}
			\Let{$error$}{0}
			\While{remainder = 0 \& error = 0}
				\Let{$error,root$}{$Newton(f_{0},f'_{0},x_{0},\epsilon,max)$}
				\If{$error = 0$}
						\Let{$roots$}{$roots \cup root$}
						\Let{$f_{1}, remainder$}{$f_{0} / (x-root)$} \hspace{0.2cm} using Ruffini/Horner rule
				\EndIf
				\If{$f_{1} = 1$}
					\State\Return{$roots$}
				\EndIf
				\Let{$f_{0}$}{$f_{1}$}
			\EndWhile
		\Return{$roots$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}
The positives of this algorithm are the efficiency of the division and simplicity, which in combination with Newton's method result in fast algorithm.

On the other hand, the division in each step produces a slight error in precision, which accumulates over the iterations and could cause larger discrepancy in the latter roots than desired.

\section{Isolation of the roots}
In this section, I present 3 algorithms which goal is to isolate the roots into smaller separate intervals, each containing exactly one root. Then, using the combination of algorithms presented in the previous chapter, approximate the roots to the allowed tolerance, thus achieving the desired result.
\subsection{Isolation of roots using Sturm's theorem}
\begin{definition}
A \textit{Sturm chain} or \textit{Sturm sequence} of a polynomial $f$ is a finite sequence of polynomials $f = f_{0}, f_{1},\ldots f_{m}$ of decreasing degree with following properties:
\begin{itemize}
  \item $f$ has simple roots
  \item if $f(\alpha)=0$, then $sgn(f_{1}(\alpha))=sgn(f'(\alpha))$
	\item if $f_{i}(\alpha)=0$ for $0 < i < m$, then $sgn(f_{i-1}(\alpha))=-sgn(f_{i+1}(\alpha))$
	\item $sgn(f_{m}(x))$ is constant for all $x$.
\end{itemize}
\end{definition}
The Sturm chain of $f$ can be created by applying Euclid's algorithm to $f(x)$ and $f'(x)$ \parencite{sturm}\parencite{vigklasphd}:
\begin{align*}
 f_{0}(x)&=f(x), \\
 f_{1}(x)&=f'(x), \\
 f_{2}(x)&=-r(f_{0},f_{1})=f_{1}(x)q_{0}(x)-f_{0}(x), \\
 f_{3}(x)&=-r(f_{1},f_{2})=f_{2}(x)q_{1}(x)-f_{1}(x), \\
 \vdots \\
 f_{m}(x)&=-r(f_{m-2},f_{m-1})=f_{m-1}(x)q_{m-2}(x)-f_{1}(x), \\
0&=-r(f_{m-1},f_{m}).
\end{align*}
The $r(f_{i},f_{i+1})$ represents the remainder and the $q_{i}$ represents the quotient of the polynomial long division $\frac{f_{i}}{f_{i+1}}=q_{i}(x)f_{i}(x)+r(f_{i},f_{i+1})$ for every $0 \leq i < m$. As the degree of the remainder $deg(r(f_{i},f_{i+1}))$ is at most $deg(f_{i+1})-1$, the maximum length of the chain is $m \leq deg(f_{0})$.

\begin{definition}\label{def:signvar}
Let $\lambda_{0}, \lambda_{1}, \lambda_{2},\ldots$ be a finite or infinite sequence of real numbers. Suppose $a < b$; $a, b\in\N$ and the following condition holds
\begin{align}
	sgn(\lambda_{a}) = -sgn(\lambda_{b}) \land \forall i, a < i < b: \lambda_{i} = 0.
\end{align}
Then this is called a \textit{sign variation} or \textit{sign chance} between $\lambda_{a}$ and $\lambda_{b}$.
\end{definition}

To provide the Sturm's theorem, let $\sigma(f,\xi)$ represent the number of sign changes in the sequence $f_{0}(\xi), f_{1}(\xi),\ldots,f_{m}(\xi)$, where $f_{0}, f_{1},\ldots,f_{m}$ is Sturm chain of $f$.

\begin{theorem}[Sturm{\parencite{sturm}}]
Assume $f(x)$ is a real polynomial without a root of multiplicity greater than one and $a, b$ are real numbers such that $a<b, f(a)\neq{0}$, $f(b)\neq{0}$. Then the number of real roots in interval $(a,b]$ is equal to $\sigma(f,a) - \sigma(f,b)$.
\end{theorem}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\begin{remark}
  The theorem can be strengthened(as proven in \cite{sturmGeneral}) to any polynomial, including the ones with roots with higher multiplicity. Then the difference of sign changes $\sigma(f,a) - \sigma(f,b)$ provides the number of \textit{distinct} roots in $(a,b]$.
\end{remark}
Using this theorem and combining it with Newton's method and bisection, I created the algorithm \ref{alg:rfs}.
\begin{algorithm}
  \caption{Sturm's algorithm
    \label{alg:sturm}}
  \begin{algorithmic}[1]
    \Require{$f, f'$ polynomial function and its first derivative, $a, b$ interval bounds}
    \Statex
    \Function{Sturm}{$f, f', a, b$}
			\Let{$\sigma(a)$}{0}
			\Let{$\sigma(b)$}{0}
			\Let{$f_{0}$}{$f$}
			\Let{$f_{1}$}{$f'$}
			\If{$sgn(f_{0}(a)) \neq sgn(f_1(a))$}
				\Let{$\sigma(a)$}{$\sigma(a)+1$}
			\EndIf
			\If{$sgn(f_{0}(b)) \neq sgn(f_1(b))$}
				\Let{$\sigma(b)$}{$\sigma(b)+1$}
			\EndIf
		\While{$f_1 \neq 0$}
			\Let{$r$}{$f_{0} \hspace{0.2cm} mod \hspace{0.2cm} f_{1}$}
			\Let{$f_{0}$}{$f_{1}$}
			\Let{$f_{1}$}{$-r$}
			\If{$sgn(f_{0}(a)) \neq sgn(f_1(a))$}
				\Let{$\sigma(a)$}{$\sigma(a)+1$}
			\EndIf
			\If{$sgn(f_{0}(b)) \neq sgn(f_1(b))$}
				\Let{$\sigma(b)$}{$\sigma(b)+1$}
			\EndIf
		\EndWhile
		\State\Return{$\sigma(a)-\sigma(b)$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}
\begin{algorithm}
  \caption{Root finding (sturm) algorithm
    \label{alg:rfs}}
  \begin{algorithmic}[1]
    \Require{$f, f'$ polynomial and its derivative, $a$, $b$ interval bounds, $\epsilon$ precision error, $max$ number of maximum iterations}
    \Statex
    \Function{RootFindingSturm}{$f, f', a, b, \epsilon, max$}
			\Let{$roots$}{$\emptyset$}
			\Let{$root\_count$}{$Sturm(f, a, b)$}
			\If{$root\_count > 0$}
				\If{$root\_count = 1$}
					\Let{$error,root$}{$Newton(f,f',a,b,\epsilon,max)$}
					\If{$error = 0$}
						\Let{$roots$}{$roots \cup root$}
						\State\Return{$roots$}
					\EndIf
				\EndIf
				\Let{$c$}{$\frac{a+b}{2}$} 
				\Let{$roots$}{$roots \cup RootFindingSturm(f, f', a, c, \epsilon, max)$}
				\Let{$roots$}{$roots \cup RootFindingSturm(f, f', c, b, \epsilon, max)$}
			\EndIf
			\Return{$roots$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}
%\begin{example}
%
%\end{example}
\FloatBarrier

The algorithm has several deficiencies.

The major deficiency of this algorithm is the time complexity of and evaluating the polynomials of the chain in points $a$ and $b$ to calculate the difference in sign changes. Since there can be up to $n$ polynomials in the chain, where $n$ is the degree of $f$, this can result up to $O(n^2)$ multiplications and additions in every iteration of bisection.

Another issue is that while creating the Sturm's chain, depending on the implementation, if an error is introduced during the polynomial long division, this leads to snowball effect in the subsequent divisions, which may result in a different number of the polynomials in the chain computed than the actual number of the polynomials in chain. This may happen if the coefficients are represented as floating-point numbers with set precision, instead of representing them as fractions of integers (which is often impractical). This can result into the number of sign changes $\sigma(\xi)$ being incorrect, leading to the number of roots located in the interval $(a,b]$ being wrong and causing some roots not being found. However, this can be avoided by sufficiently reducing the tolerance of error.

\subsection{Isolation of roots using Vincent's theorem}
Vincent published his theorem \parencite{vincentT1}\parencite{vincentT2} in 19th century, but because of the appearance of Sturm's theorem, it was forgotten until the end of 20th century, when it was brought back by \parencite{akritasphd}. This led to the creation of a second version of the theorem in \parencite{vincentBis}.
\begin{theorem}[Vincent(continued fractions version) \parencite{vincentT1}\parencite{vincentT2}]
\label{theor:v1}
Given a polynomial function $f(x)$ with rational coefficients and without multiple roots, if one sequentially performs replacements of the form
\begin{align}
x\leftarrow \alpha_{1} + \frac{1}{x}, x\leftarrow \alpha_{2} + \frac{1}{x}, x\leftarrow \alpha_{3} + \frac{1}{x},\ldots,
\end{align}
where $\alpha_{1}\geq 0$ is an arbitrary nonnegative integer and $\alpha_{2}, \alpha_{3},\ldots$ are arbitrary positive integers, $\alpha_{i} > 0, i > 1$, then the resulting polynomial has either zero sign variations (\ref{def:signvar}) or one sign variation. In the former case, there are no positive roots, whereas in the latter case, the equation has exactly one positive root, which is represented by the continued fraction
\begin{align}
\alpha_{1}+\frac{1}{\alpha_{2}+\frac{1}{\alpha_{3}\frac{1}{\ddots}}}.
\end{align}
\end{theorem}
The negative root can be treated the same way, by simply performing substitution $x \leftarrow -x$ on $f(x)$.
The requirement that the polynomial function $f(x)$ has no multiple roots (i.e. roots with multiplicity higher than one) does not restrict the generality of the theorem, because in the case that polynomial contains multiple roots, we can first apply square-free factorization and then isolate the roots of the square-free factor \parencite{vincentA}.

\begin{theorem}[Vincent(bisection version) \parencite{vincentBis}]
\label{theor:v2}
Let $f(x)$ be a real polynomial of degree $n$ which has only simple roots. It is possible to determine a positive quantity $\delta$ so that for every pair of positive real numbers $a, b$ with $\left|b - a\right| < \delta$, every transformed polynomial of the form
\begin{align}
\phi(x) = (1 + x)^{n} f\left(\frac{a + bx}{1 + x}\right)
\end{align}
has exactly 0 or 1 sign variations. The second case is possible if and only if $f(x)$ has a single root within $(a,b)$.
\end{theorem}
As in the previous case, the negative roots can be handled by substituting $x \leftarrow -x$ and the condition of $f(x)$ only containing simple roots can be resolved by applying square-free factorization.

Using this theorem, we can create a simple test that gives us the upper bound on the positive roots inside the interval $(a,b)$:
\begin{equation}
\sigma_{\left(a,b\right)}(f) = \sigma\left((1 + x)^{n} f\left(\frac{a + bx}{1 + x}\right)\right) = \sigma_{\left(b,a\right)}(f),
\label{eq:a_b_test}
\end{equation}
where $\sigma(\xi)$ is the number of sign variations of the sequence of coefficients $a_{0}, a_{1},ldots,a_{n}$.

Here are presented the two major algorithms, each based on the different version of the theorem.
\subsubsection{\textbf{Vincent–Akritas–Strzeboński (VAS)} \parencite{vas}}
Let the \textit{M{\"o}bius transformation} \parencite{mobius}
\begin{equation}
M = \frac{ax+b}{cx+d} \tag*{$a,b,c,d\in\N;ad-bd\neq 0$}
\label{eq:mobius}
\end{equation}
represent the finite continuous fraction that substitutes $x$ in $f(x)$ such that the substitution results in the transformed polynomial 
\begin{equation}
\label{eq:mobius2}
g(x)=(cx+d)^{deg(f)}f\left(\frac{ax+b}{cx+d}\right)
\end{equation} with one sign change. Then the positive root of $f(x)$ (located in $(0,\infty)$) corresponds to the root of $g(x)$ in $(min(\frac{a}{c},\frac{b}{d}), max(\frac{a}{c},\frac{b}{d}))$. Thus, to isolate the positive roots, one needs to compute $a,b,c,d$ such that they result in the polynomial \eqref{eq:mobius2} with one sign change.
The Descartes' rule of signs \parencite{descart}, states that the number of positive roots of $f(x)$ is equal to the number of sign changes of the coefficients $\sigma(f)$ or less than it by an even number. We can use this simple test to determine (line 2 and 4) if the $f$ has exactly 0 or 1 positive root or if it has more. If there are more, we first transform the polynomial by moving the lowest positive closer to 0 and then we split the polynomial into two transformed ones - the first representing the interval between $(0,1)$ and the second representing the interval $(0,\infty)$. Every time we perform substitution, we perform corresponding substitution on M. This allows us to retroactively compute the corresponding interval in the original polynomial.

Thus the algorithm (\ref{alg:vas}) returns intervals such each contains exactly 1 positive root. To isolate the negative intervals, we simply substitute $x\leftarrow -x$, execute the algorithm again and then unite the sets of intervals. Then we can use one of the approximation algorithms (e.g. Newton's algorithm (\ref{alg:newton})) and approximate the root in every interval.

The lower bound ($lb$) of $f$ can be computed by computing upper bound ($ub$) of $g(x)=x^{deg(f)}f(\frac{1}{x})$ and then $lb_{f} = \frac{1}{ub_{g}}$, as shown \parencite{vigklasphd}. \parencite{vigklasphd} also provides 2 efficient upper bounds.

\begin{algorithm}
  \caption{VAS
    \label{alg:vas}}
  \begin{algorithmic}[1]
    \Require{$f$ polynomial function, $M$ m{\"o}bius transformation}
    \Statex
    \Function{VAS}{$f, \epsilon$}
			\If{$\sigma(f) = 0$}
				\State\Return{$\emptyset$}
			\EndIf
			\If{$\sigma(f) = 1$}
				\Let{$a$}{$min(M(0), M(\infty))$} where $M(\infty)=\frac{a}{c}$ if $c\neq 0$
				\Let{$b$}{$max(M(0), M(\infty))$} and if $c=0$ then $M(\infty)=$an upper bound on the positive roots
				\State\Return{$\{(a,b)\}$}
			\EndIf
			\Let{$lb$}{a lower bound on the positive roots of $f(x)$}
			\If{$lb \geq 1$}
				\Let{$f$}{$f(x+lb)$}
				\Let{$M$}{$M(x+lb)$}
			\EndIf
			\Let{$f_{01}$}{$(x+1)^{deg(f)}f(\frac{1}{x+1})$}
			\Let{$M_{01}$}{$M(\frac{1}{x+1})$}
			\Let{$f_{1\infty}$}{$f(x+1)$}
			\Let{$M_{1\infty}$}{$M(x+1)$}
			\Let{$m$}{$M(1)$}
			\If{$f(1)\neq 0$}
				\State\Return{$VAS(f_{01},M_{01})\cup VAS(f_{1\infty},M_{1\infty})$}
			\Else
				\State\Return{$VAS(f_{01},M_{01})\cup VAS(f_{1\infty},M_{1\infty})\cup \{(m,m)\}$}
			\EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\subsubsection{\textbf{Vincent–Collins–Akritas (VCA)} \parencite{vca}}
While the previous algorithm required computing the upper and lower bound on the positive roots, this algorithm allows the user to enter the desired search interval $(a,b)$. This was not possible in the previous case, as the Descartes' rule gives upper bound on all positive roots. However, using the test \eqref{eq:a_b_test} (or rather a special case of it called Budan's test \parencite{budan}) allows us to specify the interval we are interested in locating the roots.

The Budan's test \parencite{budan} is a version of the test \eqref{eq:a_b_test} where $a=0, b=1$, which results in
\begin{equation}
\sigma_{\left(0,1\right)}(f) = \sigma\left((1 + x)^{deg(f)} f\left(\frac{1}{1 + x}\right)\right).
\label{eq:0_1_test}
\end{equation}
If $\sigma_{\left(0,1\right)}(f)$ is equal to 0, there is no root located within $(0,1)$, while if $\sigma_{\left(0,1\right)}(f) = 1$, there is precisely 1 root. Any number higher than 1 gives upper bound on the roots located within the interval $(0,1)$ such that the exact number is equal to the $\sigma_{\left(0,1\right)}(f)$ or less by an even number (multiplicities counted), meaning if the upper bound is equal to even number, they may be zero roots in the interval.

The first step is then to make sure that all roots of $f(x)$ we want to isolate are within the entered interval $(a,b)$. To do this, we first perform a substitution $x\leftarrow x+a$ on $f$ to obtain $f_{a}$. Then we perform a substitution $x\leftarrow x*(b-a)$ on $f_{a}$ to obtain $f_{ab}$. This gives us a bijection of the interval $(a,b)$ of $f$ on $(0,1)$ of $f_{ab}$. We use this as the initial input of \ref{alg:vca}, along with $a, b$.

Afterwards we perform the Budan's test \eqref{eq:0_1_test} on the transformed polynomial $f_{ab}$. If there is 0 or 1 root, we can finish the search. Otherwise we split the $f_{ab}$ into two polynomials, $f_{0\frac{1}{2}}$ and $f_{\frac{1}{2}1}$ such that there is bijection between the intervals $(0,\frac{1}{2})$ and $(\frac{1}{2},1)$ of $f_{ab}$ and the intervals $(0,1)$ of $f_{0\frac{1}{2}}$ and $f_{\frac{1}{2}1}$, respectively. Then we can use these new polynomials recursively.

If the original polynomial $f$ is not square-free, this, in theory, will cause an endless recursion of the algorithm, since the Budan's test will always return a number greater than one. However, there are 2 solutions to this problem:
\begin{enumerate}
	\item perform a square free factorization on $f$,
	\item select a tolerance $\epsilon$ such that if $\left|a-b\right|<\epsilon$ algorithm ends.
\end{enumerate}
Since the second solution is much simpler and in line with the fact that we are approximating the roots to a certain precision, we apply it to obtain the algorithm \ref{alg:vca}. If the upper bound is odd, there are either multiple roots or a single root with odd multiplicity. If there are multiple roots, then this does not influence the result, since they are closer to each other than the desired tolerance and thus we are unable to distinguish them. If the upper bound is even, then there is no guarantee that the interval contains root. This does not effect the final result, however, since afterwards we execute Newton's algorithm \ref{alg:newton} in every interval, which gives us the desired approximation only if the root is located within the interval.

\begin{algorithm}
  \caption{VCA
    \label{alg:vca}}
  \begin{algorithmic}[1]
    \Require{$f$ polynomial function, $a,b$ searched interval, $\epsilon$ tolerance}
    \Statex
    \Function{VCA}{$f, a, b, \epsilon$}
			\Let{$bud$}{$\sigma\left((1 + x)^{deg(f)} f\left(\frac{1}{1 + x}\right)\right)$}
			\If{$bud = 0$}
				\State\Return{$\emptyset$}
			\EndIf
			\If{$bud = 1$}
				\State\Return{$\{(a,b)\}$}
			\EndIf
			\If{$\left|a-b\right|<\epsilon$}
				\State\Return{$\{(a,b)\}$}
			\EndIf
			\Let{$f_{0\frac{1}{2}}$}{$(2)^{deg(f)}f(\frac{x}{2})$}
			\Let{$f_{\frac{1}{2}1}$}{$(2)^{deg(f)}f(\frac{x+1}{2})$}
			\If{$f(\frac{1}{2})\neq 0$}
				\State\Return{$VCA(f_{0\frac{1}{2}}, a, \frac{a+b}{2}, \epsilon)\cup VCA(f_{\frac{1}{2}1}, b, \epsilon)$}
			\Else
				\State\Return{$VCA(f_{0\frac{1}{2}}, a, \frac{a+b}{2}, \epsilon)\cup VCA(f_{\frac{1}{2}1}, b, \epsilon) \cup \{[\frac{a+b}{2},\frac{a+b}{2}]\}$}
			\EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\chapter{Implementation}
The computing the chain can be done only once, if we create a class representing the chain instead of calling the function $Sturm$ on line 3 of algorithm \ref{alg:rfs} and computing it every time the function is called).
ake boundy som pouzil na VAS
spomenut ze substitucie sa daju robit otocenim koeficientov
\chapter{Experiments}

\printbibliography[heading=bibintoc]


  \makeatletter\thesis@blocks@clear\makeatother
  \phantomsection %% Print the index and insert it into the
  \addcontentsline{toc}{chapter}{\indexname} %% table of contents.
  \printindex

\appendix %% Start the appendices.
\chapter{An appendix}
Here you can insert the appendices of your thesis.

\end{document}
